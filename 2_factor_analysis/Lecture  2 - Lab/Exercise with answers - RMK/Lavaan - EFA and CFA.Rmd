---
title: 'Lab: EFA and CFA'
output:
  html_document: 
    toc: true
    toc_float: true
    number_sections: true
    toc_depth: 2
  pdf_document: default
editor_options:
  chunk_output_type: console
---

<style>
body {
  width: 100%;
  margin: 0 auto;
}
</style>


```{r setup, include = FALSE}
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 80), tidy = TRUE)
knitr::opts_chunk$set(comment = NA, warning = FALSE)
options(width = 1200) # width console output
```

Today’s lab meeting consists of three exercises.


Practical information:

- All the data files for these exercises can be found at the course platform (i.e., UU learning platform). Make sure to download and unzip the .zip file. The folder containing the data files will probably be your working directory.


Short description of exercises:

- In Exercise 1, you replicate what was done in the lecture with respect to the basics of factor analysis. 

- Exercises 2 and 3 are additional exercises on EFA and CFA, respectively.

&nbsp;


<details><summary><b>Click to show helpful lavaan notation</b></summary>

&nbsp;

**Helpful notation (in factor analysis)**

operator | meaning
--|:----
=~  | indicator; used for latent variable to observed indicator in factor analysis measurement models.
~~  | (residual) covariance.
~1  | intercept or mean (e.g., x ~ 1 estimates the mean of variable x).
~ (in regression) | is regressed on.
1*  | fixes parameter or loading to one.
NA* | frees parameter or loading (useful to override default marker-variable method in CFA).
a*  | labels the parameter as ‘a’, used for model constraints.



**General `lavaan` specification**

**Latent variable definitions:**

* f1 =~ y1 + y2 + y3 
* f2 =~ y4 + y5 + y6 
* f3 =~ y7 + y8 + y9 + y10

**Regression:**

* y1 + y2 ~ f1 + f2 + x1 + x2
* f1 ~ f2 + f3
* f2 ~ f3 + x1 + x2

**Variances and covariances:**

* y1 ~~ y1 
* y1 ~~ y2 
* f1 ~~ f2

**Intercepts:**

* y1 ~ 1 
* f1 ~ 1

</details>

&nbsp;


# Exercise 1: Replicate SAPI results from the lecture 

The file **Sapi.txt** contains the data that was used to conduct the EFA and CFA as presented in the lecture. 

Note: To avoid issues, make sure that you specify -999 as missing data.

&nbsp;


## Exercise 1.1: Reflective (confirmatory) Factor Model

&nbsp;


### Exercise 1.1a 

Inspect the correlations for the items of interest: Q77 Q84 Q170 Q196. 

*Why would you inspect these before doing a CFA?*


<details><summary><b>Click to show answers</b></summary>

&nbsp;

**Installing and loading Lavaan**

The lavaan package is available on CRAN. Therefore, to install lavaan, simply start up R (studio), and type/copy in the R console:
```{r, eval=FALSE}
if (!require("lavaan")) install.packages("lavaan", dependencies = TRUE)
```
Note that you only have to install a package once. 

Next, you need to load the package, which needs to be done every time you re-open R:
```{r}
library(lavaan) 
```
This also is a check whether the installation was successful: In that case, a start-up message will be displayed showing the version number (always report this in your papers), and a reminder that this is free software. 
If you see this message, you are ready to start.


**Read in data**

To load the data into the R environment, run the following:
```{r, message=F, warning=F}
data_sapi <- read.table("Sapi.txt", header = T)
```
Note: The data file needs to be in the same folder location that you are working in. Otherwise, you can tell R where to find it by copying the folder path. 

Now the data is loaded, you need to find missing values and set them to "NA". In this dataset, "-999" denotes a missing value:

```{r, message=F, warning=F}
data_sapi[sapply(data_sapi, function(x) as.character(x) %in% c("-999") )] <- NA 
```
If you ‘forget’ to tell  R this, these values will be treated as being observed. 


**Item correlations**
 
Before we specify any model, we need to check the correlations among the items of interest. These items are: *Q77 Q84 Q170 Q196*. The correlation table gives the correlations between pairs of items, that is, the standardized covariances between a pair of items, equivalent to running covariances on the Z-scores of each item. In R, this can be obtained via:

```{r, message=F, warning=F}
# package for correlations
if (!require("Hmisc")) install.packages("Hmisc", dependencies = TRUE)
library(Hmisc)

# rcorr() accepts matrices only
res <- rcorr(as.matrix(data_sapi[, c(9:10, 12:13)]))
# Correlation matrix (rounded to 3 decimals)
round(res$r, 3) 
```

Bear in mind that 

- The correlation for Items X and Y is the same as for Y and X. 

- The diagonal elements are always one because an item is always perfectly correlated with itself. 

- The magnitude of a correlation is determined by the absolute value of the correlation.


From the R output, we see that the items have magnitudes ranging from 0.2 to 0.6. In psychology and the social sciences, the magnitude of correlations above 0.30 is considered a medium effect size. Due to relatively high correlations among many of the items, these data would be a good candidate for factor analysis. 

The goal of factor analysis is to model the interrelationships between many items with fewer unobserved or latent variables.

</details>

&nbsp;


### Exercise 1.1b

Replicate the results for the one-factor extraversion model presented in the lecture slides (using the item Q77, Q84, Q170, and Q196). 

One can use the function `lavaan()` but also the function `cfa()` for this (the answers below use the latter).


<details><summary><b>Click to show answers</b></summary>

&nbsp;

With factor analysis, you can determine the extent to which items designed to measure a particular factor (in this case, extraversion) actually do so. It is the ‘true’ correlation between an indicator and a factor. 

In `lavaan`, the operator for the latent variable definition is: "=~". This denotes 'is measured by' (cf., BY in Mplus). Stated otherwise, it represents the indicator equation where the latent variable is on the left and the indicators (or observed variables), separated by + signs, are to the right of that symbol, where the names come from the data set.

**Specify the model**

```{r, message=F, warning=F}
# Model statement: one factor, four items, default marker method
model.CFA <- 'Extraversion =~ Q77 + Q84 + Q170 + Q196' 
```

**Fit the model**

This code runs a confirmatory factor analysis using the `cfa()` function, which is actually a wrapper for the `lavaan()` function (which we used on Day 1).

```{r, message=F, warning=F}
# Fit the model:
fit_CFA <- cfa(model.CFA, data=data_sapi,
               missing='fiml', fixed.x=F) # Specify FIML 
```

**Model summary**

Inspect the model summary. This requests textual output, listing for example:

* the estimator used
* the number of free parameters
* the test statistic
* estimated means
* loadings 
* variances

```{r, message=F, warning=F}
# Output
summary(fit_CFA) 

# Alternative
summary(fit_CFA, fit.measures=TRUE)
```

</details>

&nbsp;


## Exercise 1.2: Scaling your latent variable 

&nbsp;

### Exercise 1.2a

Is the latent variable scaled via the variance of the factor (reference group scaling) or via the factor loadings (marker variable scaling)?

Hint: You can use the function `lavInspect()` to obtain insight.


<details><summary><b>Click to show answers</b></summary>

&nbsp;

```{r, message=F, warning=F}
# Technical output - comparable to TECH1 in Mplus
lavInspect(fit_CFA)
```

From this one can see what type of scaling is used. Because of:

```{r, message=F, warning=F}
lavInspect(fit_CFA)$lambda
```
one can see that the first factor loading is not estimated ('Q77       0'), since it is fixed (from the estimates you can see that it is fixed to 1.000). Hence, the marker variable method is used.

</details>

&nbsp;


### Exercise 1.2b

Adjust the model such that you scale in the other way. 

That is, if part a) was scaled by fixing a factor loadings to 1, then free this factor loading and fix the factor variance to 1; If part a) was scaled by fixing the factor variance to 1, then free the factor variance and fix one of the factor loadings to 1.


<details><summary><b>Click to show answers</b></summary>

&nbsp;

By default, `lavaan` chooses the marker method. To use the 'var std' method (i.e., reference group scaling), one should make use of the following:

* To free a parameter, put 'NA*' in front of the parameter to be freed:

  The syntax 'NA*Q77' frees the loading of the first item because by default marker method fixes it to one.

* To fix a parameter to 1, put '1*' in front of the parameter to be fixed:

  The syntax Extraversion ~~ 1*Extraversion fixes the variance of the factor Extraversion to one.


```{r, message=F, warning=F}
# Specify model:
model.CFA_RefGr <- '
 Extraversion =~ NA*Q77 + Q84 + Q170 + Q196
 Extraversion ~~ 1*Extraversion
'
# Fit model:
fit_CFA_RefGr <- cfa(model.CFA_RefGr, data=data_sapi,
               missing='fiml', fixed.x=F) # Specify FIML 

# Get model summary:
summary(fit_CFA_RefGr)
```

Alternatively, you can use std.lv=TRUE and obtain the same results. Note that the Chi-square fit results are exactly the same across the models, as will be explained below. 

For better interpretation of the factor loadings, you should request the standardized solutions. Therefore, one may want to request the summary of the marker method but also specify that standardized=TRUE. Then you obtain both results:

```{r, message=F, warning=F}
summary(fit_CFA, standardized=TRUE)
```

Now, there are two additional columns: Std.lv and Std.all (cf. STD and STDYX in Mplus).

* **Std.all:** standardizes the factor loadings by the standard deviation of both the predictor (i.e., the factor, X) and the outcome (i.e., the item, Y).
* **Std.lv:**  standardizes the factor loadings by the predictor (i.e., the factor, X).

Comparing the unstandardized estimates with the Std.lv ones: The loadings and variance of the factors are different but the residual variances are the same.

</details>

&nbsp;


### Exercise 1.2c

Bonus: Scale via the factor loadings again, but now fix the factor loading of Q196 to 1, and free the factor loading that was previously fixed to 1.

<details><summary><b>Click to show answers</b></summary>

&nbsp;


**Fix other factor loadings**

Remember:

* 1* fixes a parameter or loading to one
* NA* frees a parameter or loading (useful to override default marker method)

In case you want, in the Marker variable method, another factor loading (here, the last one) to be 1, use:
```{r, message=F, warning=F}
# Model statement:
model.CFA_last <- '
 Extraversion =~ NA*Q77 + Q84 + Q170 + 1*Q196
'

# Fit model:
fit_CFA_last <- cfa(model.CFA_last, data=data_sapi,
                     missing='fiml', fixed.x=F) # Specify FIML 

# Model summary:
summary(fit_CFA_last)
```

&nbsp;


### Exercise 1.2d

Based on the output of the models run in 1.1b, 1.2b, and 1.2c, what do you notice about the Chi-square ($\chi^2$)), degrees of freedom, $p$-values, factor loadings, and factor variance for the different methods?


<details><summary><b>Click to show answers</b></summary>

&nbsp;

The first thing you may notice is that all Chi-square fit results are exactly the same across the models. This is because the different ways of scaling all result in equivalent statistical models. You do not estimate anything more or less with any of the three scaling methods.

The second thing you may notice is that the values of the loadings have changed. However, loadings that are relatively large (or small) in one model, are also relatively large (or small) in the other models. For example, in each model, the loading for Q77 is approximately 1.4 times as large as the loading of Q84.

The take home message is that it does not matter how you scale, the information that you obtain is the same. 	

</details>

&nbsp;


## Exercise 1.3: Model fit 

Inspect the model fit. Focus on the Chi-square, CFI, TLI, RMSEA, and SRMR. 

You can use `summary()` or `fitMeasures()`.

<details><summary><b>Click to show an extra hint</b></summary>

&nbsp;

One can use `summary(<fit>, fit.measures=TRUE)` and/or `fitMeasures(<fit>)` and/or `fitMeasures(<fit>, c("cfi", "tli", "rmsea","srmr"))`.

</details>

&nbsp;


<details><summary><b>Click to show answers</b></summary>

&nbsp;

Four measures of model fit are:

1. The 'chisq' or 'the Test Statistic for the Model Test User Model' is the Chi-square statistic obtained from the maximum likelihood statistic. 

2. CFI is the Comparative Fit Index.
    * Values can range between 0 and 1.
    * Values greater than 0.90, or conservatively 0.95, indicate good fit.

3. TLI is the Tucker Lewis Index
    * Values can range between 0 and 1. If it is greater than 1, it should be rounded to 1.
    * Values greater than 0.90 indicating good fit. 
    * If the CFI and TLI are less than one, the CFI is always greater than the TLI.

4. RMSEA is the root mean square error of approximation. 
    * You also obtain a p-value of close fit, that is, for the test whether RMSEA < 0.05.
    * If you reject the model, it means your model is not a close fitting model.
    
You can obtain these using the following code:

```{r, message=F, warning=F}
## One method:
#summary(fit_CFA, fit.measures=TRUE) # many model fit measures
#
## Alternative method:
#fitMeasures(fit_CFA) # many model fit measures

#You can ask for specific model fit measures by using:
fitMeasures(fit_CFA, c("chisq", "df", "pvalue", "cfi", "tli", "rmsea","srmr")) 
```

Notes on Chi-square: 

* Failing to reject the model is good for the model at hand because it implies failing to disprove that the model is bad. Here, we reject the null hypothesis that the model fits the data. 

* It is well documented in CFA and SEM literature that the Chi-square is often overly sensitive in model testing especially for large samples. 

* David Kenny states that, for models with 75 to 200 cases, the Chi-square is a reasonable measure of fit; but for 400 cases or more it is nearly almost always significant. Our sample size is 1000.

* Kline (2016) notes the N:q rule, which states that 

    - the sample size (N) should be 'in line with' the number of parameters in your model (q). 

    - the recommended ratio is 20:1.

    - this means that if you have 12 parameters, you should have N=240.

   Additionally, a sample size less than 100 is almost always untenable according to Kline.

* The larger the Chi-square value, the larger the difference between the sample implied covariance matrix and the sample observed one; and the more likely you will reject your model. 


Other fit measures:

* For CFI, value slightly below the recommended cut-off criterion of 0.95. 

* For TLI, value below the recommended cut-off criterion of 0.90.

* The RMSEA is above the cut-off of 0.05. 

* The SRMR points to a good model fit (SRMR < 0.08).


Overall, the model does not seem to fit well.



**Plotting the results**

You can plot the results of a fit object using `lavaanPlot()`, `tidySEM()`, or `semPlot()`. 

```{r, message=F, warning=F, results='hide'}
# Install this package first (once):
if (!require("lavaanPlot")) install.packages("lavaanPlot") 
library(lavaanPlot)

# Plot:
lavaanPlot(model = fit_CFA, node_options = list(shape = "box", fontname = "Helvetica"), 
           edge_options = list(color = "grey"), coefs = T, stand = T, covs = T)
```

```{r, message=F, warning=F, results='hide'}
# Alternative:
if (!require("tidySEM")) install.packages("tidySEM") # install this package first (once)
library(tidySEM)
graph_sem(fit_CFA)
```

```{r, message=F, warning=F, results='hide'}
# Another alternative:
if (!require("semPlot")) install.packages("semPlot") # install this package first (once)
library(semPlot)
semPaths(fit_CFA, "par", weighted = FALSE, nCharNodes = 7, shapeMan = "rectangle",
         sizeMan = 8, sizeMan2 = 5)
```

</details>

&nbsp;



## Exercise 1.4: Test equality of factor loadings

Based on the factor loadings, items do not seem to be equally reflective of Extraversion, except for Q84 and Q196. To improve the fit of the model, one can test whether these factor loadings are equal (resulting in estimating less parameters). One can use a Wald test to test whether two factor loadings can be considered equal.

&nbsp;

### Exercise 1.4a

Test whether the factor loadings of Q84 and Q196 are the same.

Hint: For a `lavaan` object (with label names!) one can do this with lavTestWald(fit, constrains = '\<insert equality restriction\>').


<details><summary><b>Click to show answers</b></summary>

&nbsp;

**Specify the model**

```{r, message=F, warning=F}
# Specify model:
model.CFA_equal <- '
 # model with labeled parameters
 Extraversion =~ Q77 + a*Q84 + Q170 + b*Q196
'
```

**Fit the model**

```{r, message=F, warning=F}
# Fit model:
fit_CFA_equal <- cfa(model.CFA_equal, data=data_sapi,
                     missing='fiml', fixed.x=F) # Specify FIML 
```

**Conduct the Wald test**

```{r, message=F, warning=F}
# Wald test
lavTestWald(fit_CFA_equal, constraints = 'a == b')
```

</details>

&nbsp;


### Exercise 1.4b 

What does the test imply?

<details><summary><b>Click to show answers</b></summary>

&nbsp;

The Wald test with df=1 is 0.362, with $p$ = 0.547. Thus, we do not reject the null that the two loadings are equal. Hence, we conclude that there is no evidence for a difference between a and b, that is, the factor loadings of Q84 and Q196 are the same.

In case one wants the model results for which the loadings are the same: 

```{r, message=F, warning=F}
# Specify model:
model.CFA_equal <- '
 # model with labeled parameters
 Extraversion =~ Q77 + a*Q84 + Q170 + b*Q196
 #
 # constraints
 a == b 
'
# Note: a* labels the parameter as ‘a’, used for model constraints.
#
# Alternatively:
model.CFA_equal <- '
 # model with labeled parameters
 Extraversion =~ Q77 + a*Q84 + Q170 + a*Q196
'

# Fit model:
fit_CFA_equal <- cfa(model.CFA_equal, data=data_sapi,
                     missing='fiml', fixed.x=F) # Specify FIML 

# Model summary:
summary(fit_CFA_equal, standardized=TRUE, fit.measures=TRUE)
fitMeasures(fit_CFA_equal, c("cfi", "tli", "rmsea","srmr"))
```

The measures did improve, but the model still does not fit well.

</details>

&nbsp;


## Exercise 1.5: EFA vs CFA 

Specify and run the two-factor exploratory factor model (EFA) and two-factor CFA, as was shown in the lecture slides (using the items: Q44, Q63, Q76, Q77, Q84, Q98, Q170, and Q196).


<details><summary><b>Click to show answers</b></summary>

&nbsp;

**Data screening: Item correlations**

```{r, message=F, warning=F}
# Correlations for the items of interest: Q44 Q63 Q76 Q77 Q84 Q98 Q170 Q196
res <- rcorr(as.matrix(data_sapi[, c(6:13)])) # rcorr() accepts matrices only
# Correlation matrix (rounded to 3 decimals)
round(res$r, 3)
```

**Specify the two-factor EFA model**

The efa('block1') modifier indicates that the factors comprise an exploratory block (with the name block1) and thus that we do not exactly know the loading structure of the variables / items on these factors.

```{r, message=F, warning=F}
# Specify two-factor EFA model:
model.2EFA <- "
 efa('block1')*Having fun  =~ Q77 + Q84 + Q170 + Q196 + Q44 + Q63 + Q76 + Q98
 efa('block1')*Being liked =~ Q77 + Q84 + Q170 + Q196 + Q44 + Q63 + Q76 + Q98
"
# Alternative:
model.2EFA <- "
efa('block1')*Having fun  + efa('block1')*Being liked =~ 
Q77 + Q84 + Q170 + Q196 + Q44 + Q63 + Q76 + Q98
"
```

**Fit the two-factor EFA model**

```{r, message=F, warning=F}
# Fit two-factor EFA model:
fit_2EFA <- cfa(model.2EFA, data=data_sapi,
                missing='fiml', fixed.x=F) # Specify FIML 
# This estimates the EFA model (releasing the residual variances and implying the EFA constraints) using the oblique geomin rotation.

# Model summary:
summary(fit_2EFA)

lavInspect(fit_2EFA)
```

**Specify and fit the two-factor CFA model**

```{r, message=F, warning=F}
# Specify two-factor CFA model:
model.2CFA <- "
 Having fun  =~ Q77 + Q84 + Q170 + Q196
 Being liked =~ Q44 + Q63 + Q76 + Q98
"
# Fit two-factor CFA model:
fit_2CFA <- cfa(model.2CFA, data=data_sapi,
                missing='fiml', fixed.x=F) # Specify FIML 

# Model summary:
summary(fit_2CFA)
lavInspect(fit_2CFA)
```

</details>

&nbsp;


# Exercise 2: EFA 

Use the data file popular_factor.txt. Make sure to denote that 99 represents a missing value. 

&nbsp;


## Exercise 2a

Run an EFA that provides you with a 1-factor and a 2-factor solution. Interpret the results for the 2-factor solution.


<details><summary><b>Click to show answers</b></summary>

&nbsp;

**Load the data**

```{r, message=F, warning=F}
# Read in data:
data_pop <- read.table("popular_factor.txt", header = T)
# Denote missing values:
data_pop[sapply(data_pop, function(x) as.character(x) %in% c("99") )] <- NA
```

**Data screening: Item correlations**

```{r, message=F, warning=F}
# Correlations for the items of interest
res <- rcorr(as.matrix(data_pop)) # rcorr() accepts matrices only
round(res$r, 3) # Correlation matrix (rounded to 3 decimals)
```

**One-factor model**

```{r, message=F, warning=F}
# Specify one-factor EFA model:
model.1EFA <- "
 efa('block1')*F1  =~ c1 + c2 + c3 + o1 + o2 + o3
"

# Fit one-factor EFA model:
fit_1EFA <- cfa(model.1EFA, data=data_pop,
                missing='fiml', fixed.x=F) # Specify FIML 

# Model summary:
summary(fit_1EFA)
fitMeasures(fit_1EFA, c("chisq", "df", "pvalue", "cfi", "tli", "rmsea","srmr")) 
lavInspect(fit_1EFA)
```

**Two-factor model**

<!-- # TO DO not same factor loadings as in Mplus - fit etc is the same -->

```{r, message=F, warning=F}
# Specify two-factor EFA model:
model.2EFA <- "
 efa('block1')*F1  =~ c1 + c2 + c3 + o1 + o2 + o3
 efa('block1')*F2  =~ c1 + c2 + c3 + o1 + o2 + o3
"
# Fit two-factor EFA model:
fit_2EFA <- cfa(model.2EFA, data=data_pop,
                missing='fiml', fixed.x=F) # Specify FIML 

# Model summary:
summary(fit_2EFA)
fitMeasures(fit_2EFA, c("chisq", "df", "pvalue", "cfi", "tli", "rmsea","srmr"))
lavInspect(fit_2EFA)
```

One can conclude that the 2-factor structure fits well to the data:

* Chi-Square Test of Model Fit is not significant, 
* CFI/TLI is  > 0.95, and 
* RMSEA is < 0.05.

</details>

&nbsp;


## Exercise 2b

Which factor model do you prefer?

Note: you can use for example the `anova()` function for this.


<details><summary><b>Click to show answers</b></summary>

&nbsp;

**Compare the two models**

```{r, message=F, warning=F}
# Compare the one-factor model to the two-factor model

# Option 1: 
anova(fit_1EFA, fit_2EFA)
```

The Chi-square difference test is significant, implying that the null (stating that two factors are one and the same) is rejected. Additionally, the AIC and BIC for the 2-factor model is lower than for the 1-factor model. Hence, we prefer the 2-factor model.


There are alternative ways to compare the models, such as:
```{r, message=F, warning=F, results='hide'} 
# Option 2:
if (!require("performance")) install.packages("performance", dependencies = TRUE)
library(performance)

compare_performance(fit_1EFA, fit_2EFA)
```

<!-- There are more options, e.g., see the following link where fit measures are compared: -->

<!-- https://solomonkurz.netlify.app/post/2021-05-11-yes-you-can-fit-an-exploratory-factor-analysis-with-lavaan/ -->


**Plot the results**

Using `lavaanPlot()`:

```{r, message=F, warning=F}
# Plot
if (!require("lavaanPlot")) install.packages("lavaanPlot") # install this package first (once)
library(lavaanPlot)
lavaanPlot(model = fit_2EFA, node_options = list(shape = "box", fontname = "Helvetica"), 
           edge_options = list(color = "grey"), coefs = T, stand = T, covs = T)
```

Using `tidySEM()` and `semPlot()`:

```{r, message=F, warning=F}
# Alternative
if (!require("tidySEM")) install.packages("tidySEM") # install this package first (once)
library(tidySEM)
graph_sem(fit_2EFA)

# Another alternative
if (!require("semPlot")) install.packages("semPlot") # install this package first (once)
library(semPlot)
semPaths(fit_2EFA, "par", weighted = FALSE, nCharNodes = 7, shapeMan = "rectangle",
         sizeMan = 8, sizeMan2 = 5)
```

</details>

&nbsp;


# Exercise 3: CFA 

Use again data file popular_factor.txt.

&nbsp;

## Exercise 3a

Now, run a CFA with 1 factor (‘anti’) and another model with 2-factors (‘covert’ and ‘overt’) based on the 2-factor EFA model. Bear in mind that you should normally not run both an EFA and CFA on the same data set; normally, the EFA model generates a hypothesis for a next study (or for another part of your data set).

In both models, scale by fixing the factor variance, rather than by fixing a loading (in the last question, it will become clear why).


<details><summary><b>Click to show answers</b></summary>

&nbsp;

**Load the data**

```{r, message=F, warning=F}
# Read in data:
data_pop <- read.table("popular_factor.txt", header = T)
# Denote missing values:
data_pop[sapply(data_pop, function(x) as.character(x) %in% c("99") )] <- NA
```


**One-factor CFA model**

```{r, message=F, warning=F}
# Specify one-factor CFA model:
model.1CFA <- "
 anti  =~ NA*c1 + c2 + c3 + o1 + o2 + o3
 anti ~~ 1*anti
"

# Fit one-factor CFA model:
fit_1CFA <- cfa(model.1CFA, data=data_pop,
                missing='fiml', fixed.x=F) # Specify FIML 

# Model summary:
summary(fit_1CFA)
fitMeasures(fit_1CFA, c("chisq", "df", "pvalue", "cfi", "tli", "rmsea","srmr")) 
lavInspect(fit_1CFA)
```

**Two-factor CFA model**

```{r, message=F, warning=F}
# Specify two-factor CFA model:
model.2CFA <- "
 covert  =~ NA*c1 + c2 + c3
 overt   =~ NA*o1 + o2 + o3
 covert ~~ 1*covert
 overt ~~ 1*overt
"
# Fit two-factor CFA model:
fit_2CFA <- cfa(model.2CFA, data=data_pop,
                missing='fiml', fixed.x=F) # Specify FIML 

# Model summary:
summary(fit_2CFA)
fitMeasures(fit_2CFA, c("chisq", "df", "pvalue", "cfi", "tli", "rmsea","srmr"))
lavInspect(fit_2CFA)
```

</details>

&nbsp;


## Exercise 3b

Which model is to be preferred? What can you say about the model fit based on the AIC and BIC?

<details><summary><b>Click to show answers</b></summary>

&nbsp;

```{r, message=F, warning=F}
# Compare the one-factor model to the two-factor model
anova(fit_1CFA, fit_2CFA)
```
The Chi-square difference test is significant, implying that the null (stating that the two factors are one and the same) is rejected. The AIC and BIC for the 2-factor model is lower than for the 1-factor model. Hence, we prefer the 2-factor model.


```{r, message=F, warning=F}
fitMeasures(fit_1CFA, c("chisq", "df", "pvalue", "cfi", "tli", "rmsea","srmr"))
fitMeasures(fit_2CFA, c("chisq", "df", "pvalue", "cfi", "tli", "rmsea","srmr"))
```

The fit measures are better for the 2-factor CFA model. Additionally, one can conclude that the 2-factor structure fits the data well: 

- Chi-Square Test of Model Fit is not significant, 

- CFI/TLI is  > 0.95, and 

- RMSEA is < 0.05.


</details>

&nbsp;


## Exercise 3c

What is the correlation between the two factors (i.e., overt and covert)?


<details><summary><b>Click to show a hint</b></summary>

Whether the unstandardized ‘covert ~~ overt’ can be interpreted as a correlation or a covariance, depends on how the latent variable was scaled: 

- If the factor variances are fixed to 1 (i.e., when scaling is done via the factor variances), then the ‘unstandardized’ result reflects the factor correlation. 

- Otherwise they reflect the covariance.

</details>

&nbsp;


<details><summary><b>Click to show answers</b></summary>

&nbsp;

The correlation between the two factors is:
```{r, message=F, warning=F}
parameterEstimates(fit_2CFA)[15,]
```

The correlation between overt and covert is 0.431 with a standard error of 0.055. 


Consequently, the proportion of shared variance is 0.4312^2 = 0.186 = 18.6%.



**Plot the results** 

Using `lavaanPlot()`:

```{r, message=F, warning=F}
# Plot
if (!require("lavaanPlot")) install.packages("lavaanPlot") # install this package first (once)
library(lavaanPlot)
lavaanPlot(model = fit_2CFA, node_options = list(shape = "box", fontname = "Helvetica"), 
           edge_options = list(color = "grey"), coefs = T, stand = T, covs = T)
```

Using `tidySEM()` or `semPlot()`:

```{r, message=F, warning=F}
# Alternative
if (!require("tidySEM")) install.packages("tidySEM") # install this package first (once)
library(tidySEM)
graph_sem(fit_2CFA)

# Another alternative
if (!require("semPlot")) install.packages("semPlot") # install this package first (once)
library(semPlot)
semPaths(fit_2CFA, "par", weighted = FALSE, nCharNodes = 7, shapeMan = "rectangle",
         sizeMan = 8, sizeMan2 = 5)
```

</details>