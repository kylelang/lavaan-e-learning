---
title: 'Lab: regression'
output:
  html_document: 
    toc: true
    toc_float: true
    number_sections: true
    toc_depth: 2
  pdf_document: default
editor_options:
  chunk_output_type: console
---

<style>
body {
  width: 100%;
  margin: 0 auto;
}
</style>


```{r setup, include = FALSE}
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 80), tidy = TRUE)
knitr::opts_chunk$set(comment = NA, warning = FALSE)
options(width = 1200) # width console output
```


Today's lab meeting consists of three exercises.


Practical information:

- All the data files for these exercises can be found at the course platform (i.e., UU learning platform). Make sure to download and unzip the .zip file. The folder containing the data files will probably be your working directory.


Short description of exercises:

- The first exercise of today makes an explicit comparison on how certain analyses look in the software you are used to (e.g., SPSS, Excel, Mplus, SAS, or STATA) and how the corresponding analyses look in R when using the lavaan package. The aim is to understand the similarities and differences between software packages. 

- The second exercise is aimed at doing a multivariate multiple regression analysis yourself using lavaan. In addition, you will have a look at the 'technical output'.

- The third exercise is aimed at doing a path analysis yourself using lavaan. Additionally, it will make you more familiar with the 'technical output'.


# Exercise 1: Multiple regression

The data for this exercise is taken from Van de Schoot et al (2010)[^1]. The study examined the understanding of anti-social behaviour and its association with popularity and sociometric status in a sample of at-risk adolescents from diverse ethnic backgrounds (*n* = 1491, average age 14.7 years). Both overt and covert types of anti-social behaviour were used to distinguish subgroups. 

In the current exercise, you will carry out a regression analysis where you want to predict levels of socially desirable answering patterns of adolescents (*sw*) using the predictors overt and covert antisocial behaviour (*overt* and *covert*, respectively). 

**Before you will carry out this regression analysis using lavaan, you will first conduct the analysis in a program of your own choice (e.g., SPSS, Mplus, SAS, or STATA)**. By first conducting the analysis in another program, you can later check whether the results of lavaan are comparable. If they are the same, you (most certainly) know you specified everything correctly. 

In the (zipped) folder 'Data files', you can find four type of data sets, each with a different extension: popular_regr.sav, popular_regr.xlsx, popular_regr_missingsrecoded.dat, and popular_regr.txt. Use the one that fits the software you are used to work with.

Note: There are **missing data** (depending on the file you use, these are coded as system missing, -999, or -99 & -999). Make sure your software recognizes this. In R, one can for example use:

```{r, echo = TRUE, eval = FALSE}
data[sapply(data, function(x) as.character(x) %in% c("-99", "-999") )] <- NA
```

[^1]: Van de Schoot, R., Van der Velden, F., Boom, J., & Brugman, D. (2010). Can at-risk young adolescents be popular and anti-social? Sociometric status groups, anti-social behavior, gender and ethnic background. Journal of Adolescence, 33(5), 583-592. doi:10.1016/j.adolescence.2009.12.004


## Exercise 1a 
Ask for descriptive statistics and run a **correlation** analysis between the variables of interest (*sw, overt*, and *covert*). Use your software program of choice.

Note that this is part of your data screening step.


<details><summary><b>Click to show answers</b></summary>

&nbsp;

In the subfolders starting with 'Ex. 1' in the (zipped) Solutions folder, you can find the files to perform the analyses in this exercise for SPSS and for Mplus.

&nbsp;


## Exercise 1b 
Run a multiple **regression** analysis, where the dependent variable is *sw* and the independent variables are *covert* and *overt*. Also check the standardized results. Use your software program of choice.


<details><summary><b>Click to show answers</b></summary>

&nbsp;

In the subfolders starting with 'Ex. 1' in the (zipped) Solutions folder, you can find the files to perform the analyses in this exercise for SPSS and for Mplus.

&nbsp;


## Exercise 1c

Next (i.e., in the next exercise), you are going to do the same using R and lavaan. Before you can do this, take the following three steps:

- First, load (and perhaps also install) lavaan. 

- Second, read in the data. 

- Third, denote the missing values. 


<details><summary><b>Click to show answers</b></summary>

&nbsp;

**Installing and loading Lavaan**

The lavaan package is available on CRAN. Therefore, to install lavaan, simply start up R (studio), and type/copy in the R console:
```{r, eval=FALSE}
if (!require("lavaan")) install.packages("lavaan", dependencies = TRUE)
```
Note that you only have to install a package once. 

Next, you need to load the package, which needs to be done every time you re-open R:
```{r}
library(lavaan) 
```
This also is a check whether the installation was successful: In that case, a start-up message will be displayed showing the version number (always report this in your papers), and a reminder that this is free software. 
If you see this message, you are ready to start.


**Read in data**

```{r}
data_regr <- read.table("popular_regr.txt", header = T)
```
Note: The data file needs to be in the same folder location that you are working in. Otherwise, you can tell R where to find it by copying the folder path. 

If you are using data without a header, then use 'header = F' above, and add the header yourself by: 
```{r}
colnames(data_regr) <- c("respnr", "Dutch", "gender", "sw", "covert", "overt")
```

**Denote missing values**

The values -99 and -999 denote missing, so set these to NA:
```{r}
data_regr[sapply(data_regr, function(x) as.character(x) %in% c("-99", "-999") )] <- NA
```
If you forget to tell R that -99 and -999 are used to denote missing data, these values will be treated as being observed (which will probably impact the results drastically). 


&nbsp;


## Exercise 1d

Do the same as in 1a using R.

One could, for example, use the function *describe* from the *psych* package and the *rcorr* function from the *Hmisc* package, respectively.

*Interpret the output. What are your conclusions?*

*What do you think of the correlations with respect to significance, direction, and magnitude?*


<details><summary><b>Click to show answers</b></summary>

&nbsp;

**Descriptive statistics**

The (describe() function in the) psych package can be used to obtain some extended summary statistics.
```{r, message = FALSE}
if (!require("psych")) install.packages("psych", dependencies = TRUE)
library(psych) 
```

The variables of interest are: *sw, covert,* and *overt*. When we look at he column names of our data:
```{r}
colnames(data_regr)
```
we can see that those variables are the ones in columns 4 to 6, respectively.

Now, descriptive statistics, like the mean, median, min, and max, for *sw, covert,* and *overt* can be obtained via:
```{r}
describe(data_regr[, 4:6])
```


**Correlations**

Correlations for the three variables of interest, including a correlation significance test, can be obtained using the rcorr function from the Hmisc package.

```{r, message = FALSE}
if (!require("Hmisc")) install.packages("Hmisc", dependencies = TRUE)
library(Hmisc)
```

```{r}
res <- rcorr(as.matrix(data_regr[, 4:6])) # rcorr() accepts matrices only
```

Correlation matrix (rounded to 3 decimals):
```{r}
round(res$r, 3)
```
P-values (rounded to 3 decimals):
```{r}
round(res$P, 3) 
```

Sample size:
```{r}
res$n 
```

Possible conclusions:

- covert and overt are related to sw; so, it makes sense (in combination with theory) to use them as predictors.

- Their relationship is negative; so, an decrease in (c)overt relates to an increase in sw.

- covert and overt are also related (positively) to each other, but not that much (in absolute sense) that we should worry about multicolinearity.

&nbsp;


## Exercise 1e

Do the same as in 1b using R. One should use the lavaan package, for example, the lavaan function.


*Interpret the output. What are your conclusions?*

*What do you conclude with respect to significance, direction, and magnitude?*



<details><summary><b>Click to see extra information regarding lavaan notation</b></summary>

&nbsp;

**lavaan notation**

formula type                                                   | operator        | mnemonic
-------|:--:|:----
regression                                                     |       ~         | is regressed on
(residual) (co)variance                                        |       ~~        | is correlated/covaried with
intercept                                                      |       ~ 1       | intercept
latent variable definition (used in other days of the course)  |       =~        | is measured by 


**General lavaan specification**

*latent variable definitions*

f1 =~ y1 + y2 + y3 

f2 =~ y4 + y5 + y6 

f3 =~ y7 + y8 + y9 + y10

*regressions*

y1 + y2 ~ f1 + f2 + x1 + x2

f1 ~ f2 + f3

f2 ~ f3 + x1 + x2

*variances and covariances*

y1 ~~ y1 

y1 ~~ y2 

f1 ~~ f2

*intercepts*

y1 ~ 1 

f1 ~ 1

</details>

&nbsp;


<details><summary><b>Click to show answers</b></summary>

&nbsp;

**Specify the regression model**

Now, let us run a multiple regression model predicting levels of socially desirable answering patterns of adolescents (sw) using the predictors overt and covert antisocial behaviour (overt and covert, respectively). 

To run a multiple regression with lavaan, you first specify the model, then fit the model, and finally acquire the summary. To fit the model, we use the lavaan() function, which needs a model= and a data= input.

In general, a regression model is specified as follows in lavaan: 

- One needs to specify the regression equation: `dependent ~ predictor1 + predictor2  + etc`, 
where

  * The ~ is like the = in a regression equation. 
  
  * Before the ~, one states the dependent variable.
  
  * After the ~, one states the predictors, separated by the + sign.

- Additionally, one needs to specify that the error (of the dependent variable) has a variance: `dependent ~~ dependent`.
   
- If needed (as is the case here), one can specify to include an intercept: `dependent ~ 1`.

  Note: Instead of specifying intercepts and means in the model, one can include `meanstructure=TRUE` in the lavaan function (which is used when fitting your model).


Our regression model is specified as follows:
```{r}
model.regression <- '
  sw ~ overt + covert # regression
  sw ~~ sw            # residual variance
  sw ~ 1              # intercept
'
```

**Fit the regression model**

```{r}
fit_regr <- lavaan(model = model.regression, data = data_regr)
```


**Output**

```{r}
summary(fit_regr, fit.measures = TRUE, ci = TRUE, rsquare = TRUE)
```


Notes:

- By adding `standardized=TRUE`, you obtain the standardized estimates as well.

- In the output, an intercept or a residual variances is denoted by using a "." before the variable name. Here: `.sw`.
  
  If there is no dot (".") in front of the variable name, then it denotes a mean or variance, respectively.


**Parameter estimates**

You can obtain the unstandardized regression parameters via:
```{r}
coef(fit_regr) 
```

You can obtain all unstandardized parameters (with extra information) via:
```{r}
parameterEstimates(fit_regr)
```
Note that now you also obtain the confidence interval of the unstandardized estimates.


You can obtain all **standardized** parameters (with extra information) via:
```{r}
standardizedSolution(fit_regr)
```
Note that now you also obtain the confidence interval of the standardized estimates.


**Possible conclusions:**

- The regression equation (for person *i*) looks as follows:

  $sw_i = 4.997 - 0.292 * overt_i - 0.507 * covert_i +        error_i,$

  with $error_i \sim N(0,0.329).$
   
  Note: We use unstandardized estimates for this.

- The average sw (levels of socially desirable answering patterns of adolescents) when covert and overt antisocial behaviour are 0 is 4.997;

  When overt goes up with one point, then sw goes down with 0.292 points; 

  When covert goes up with one point, then sw goes down with 0.507 points.

  When you are the expert (and look at the scale of the data), you can judge whether these are relevant effects.

- The p-values and confidence intervals indicate that both effects significantly differ from zero. Thus, both variables are (statistically) meaningful predictors.
 
- The two predictors explain $(1-0.718)*100=0.282*100=28.2$ percent of sw; using the correlation of $sw$ (i.e., standardized covariance).

- When looking at the absolute values of the standardized effects, covert is a more important predictor than overt: |-0.448| > |-0.160|.

**Plot**

With lavaanPlot:
```{r, message = FALSE}
if (!require("lavaanPlot")) install.packages("lavaanPlot") 
library(lavaanPlot)
lavaanPlot(model = fit_regr, 
           node_options = list(shape = "box", fontname = "Helvetica"),
           edge_options = list(color = "grey"), coefs = T, stand = T, covs = T)
```
Alternatively, with tidySEM:
```{r, message = FALSE}
if (!require("tidySEM")) install.packages("tidySEM")
library(tidySEM)
graph_sem(fit_regr)
```

Note: We could/should have also done this before, to check whether our model specification seems to be as intended.

</details>

&nbsp;


<details><summary><b>Click to see extra output</b></summary>

&nbsp;

**Fit measures**

You can obtain many model fit measures via:
```{r}
fitMeasures(fit_regr)   
```

You can also ask for some specific model fit measures, e.g.,:
```{r}
fitMeasures(fit_regr, c("cfi", "tli", "rmsea","srmr")) 
```

To obtain the R-square, that is, $R^2$, (for sw), use:
```{r}
inspect(fit_regr, 'r2') 
```

**Technical output**

You can ask for the technical output using: 
```{r}
lavInspect(fit_regr)
``` 
More details about how to read/interpret the technical output is given in Exercise 3.

</details>

&nbsp;


<details><summary><b>Click to see extra information regarding handling missing data</b></summary>

&nbsp;

**Handling missing data**

Note that in the above listwise deletion is used. From the output, one can see that the total sample size used for the analyses is n=1343, while the total number of observations is 1491.

lavaan used listwise deletion and deleted 148 cases:

 - 145 cases because of missingness on the dependent variables and 

 -   3 cases because of missingness on the predictors.


It is better to use, for instance, FIML, as will be discusses later in the course. 

Note that in this data set, the three missing predictor cases will then be used in the analyses. This small difference in sample size will most probably result in minor difference in the output regarding the estimates. In contrast, the output regarding fit measures does change (also in case of a low amount of missing cases). For those who are interested, the reason for this is that the fit measures are now based on the joint distribution of the outcome and the predictors, while before it was based on the conditional distribution of the outcome given the predictors.


The code to use FIML is:
```{r, warning = FALSE}
model.regression <- '
  sw ~ overt + covert # regression
  sw ~~ sw            # residual variance
  sw ~ 1              # intercept
  #
  # (co)variances predictors # Needed for FIML
  overt + covert ~~ overt 
  covert ~~ covert 
  #
  # means predictors # Needed for FIML
  covert + overt ~ 1
'
fit_regr <- lavaan(model = model.regression, data = data_regr,
                       missing='fiml', fixed.x=F) # Specify FIML
```

Afterwards one can do the same as above. For example, to obtain output, use:
```{r}
summary(fit_regr, fit.measures = TRUE, ci = TRUE, rsquare = TRUE)
```



&nbsp;

# Exercise 2: Multivariate regression
In this exercise, we make use of a part (namely, the Child Development Supplement) of the Panel Study of Income Dynamics (PSID)[^2]. The goal is to investigate whether three dependent variables -- Applied problems (*APst02*), Behavioral problems (*Problems*), and Self-esteem (*Selfesteem*) -- can be predicted from the following two independent variables: digit span (*DS02*) and letters words (*LWst02*). 

Use the data file **CDSsummerschool.sav** or **CDSsummerschool.txt**. 

[^2]: Produced and distributed by the Institute for Social Research, Survey Research Center, University of Michigan, Ann Arbor, MI


## Exercise 2a
It is a good practice to draw the model you plan to estimate for yourself. 

Be very precise in how you draw the correlations. 


<details><summary><b>Click to show answers</b></summary>

&nbsp;

In the (zipped) Solutions folder, you can find the file "Ex. 2 - Path diagram.pdf", which shows such a path diagram. 

&nbsp;


## Exercise 2b 
As part of your data screening step, request for descriptive statistics and correlations using R.

Do not forget to read in your data and to denote missing values.


<details><summary><b>Click to show answers</b></summary>

&nbsp;

**Read in data**

```{r}
data_multivar <- read.table("CDSsummerschool.txt", header = T)
```

**Denote missing values**

You may have figured out that -999 denotes a missing value. So, set this to NA:

```{r}
data_multivar[sapply(data_multivar, function(x) as.character(x) %in% c("-999") )] <- NA
```

**Descriptive statistics** 

Our variables of interest are *APst02, Problems, Selfesteem, LWst02, DS02*. When we look at he column names of our data:
```{r}
colnames(data_multivar)
```
we can see that these are the first 5 columns of our data set.

The psych package can be used to obtain some extended summary statistics.

```{r, message = FALSE}
if (!require("psych")) install.packages("psych", dependencies = TRUE)
library(psych) 
```

Now, descriptive statistics, like the mean, median, min, and max, for our variables of interest can be obtained via:
```{r}
describe(data_multivar[,1:5])
```

**Correlations** 

Correlations for the five variables of interest, including correlation significance tests, can be obtained using the Hmisc package:

```{r, message = FALSE}
if (!require("Hmisc")) install.packages("Hmisc", dependencies = TRUE)
library(Hmisc)
```

```{r}
res <- rcorr(as.matrix(data_multivar[, 1:5])) # rcorr() accepts matrices only
```

Correlation matrix (rounded to 3 decimals):
```{r}
round(res$r, 3)
```
P-values (rounded to 3 decimals):
```{r}
round(res$P, 3) 
```

Sample size:
```{r}
res$n 
```

&nbsp;


## Exercise 2c 
Create lavaan code to analyze the research question (running a multivariate regression). 

In case you closed R after doing the first exercise, do not forget to load lavaan again.

*Interpret the output. What are your conclusions?*

*Also make sure that you modeled what you intended to model (Hint: Use the function ‘lavInspect()’ and/or a plot)*


<details><summary><b>Click to show answers</b></summary>

&nbsp;

For those interested, the corresponding Mplus in- and output files can be found in the subfolder 'Solutions\\Ex.2\\Mplus'. 


**Specify the regression model**

Now, let us run a multivariate regression model predicting Applied problems (*APst02*), Behavioral problems (*Problems*), and Self-esteem (*Selfesteem*), using the predictors digit span (*DS02*) and letters words (*LWst02*). 

To run a multivariate regression with lavaan, you first specify the model, then fit the model, and finally acquire the summary. To fit the model we use the lavaan() function, which needs a model= and a data= input.

The model specification is comparable to the multiple regression model in Exercise 1, but now there are multiple dependent variables:

- One does not have to specify the regression model for each dependent variable, but one can state the dependent variables separated by the + sign: `dependent1 + dependent2 + dependent3 ~ predictor1 + predictor2  + etc`.
   
  * The ~ is like the = in a regression equation. 

  * Before the ~, one states the dependent variables, separated by the + sign.

  * After the ~, one states the predictors, separated by the + sign.

- Additionally, one needs to specify that the error (of an dependent variable) has a variance (referred to as residual variance): `dependent1 ~~ dependent1`.

  Moreover, now, one also has to specify the residual covariances (i.e., the covariances of the errors of the dependent variables): `dependent1 ~~ dependent2`.

  To specify all the combinations more easily use:

  `dependent1 + dependent2 + dependent3 ~~ dependent1`

  `dependent2 + dependent3 ~~ dependent2`

  `dependent3 ~~ dependent3`

- If needed (as is the case here), one can specify to include intercepts (one for each dependent variable): `dependent1 + dependent2 + dependent3 ~ 1`. 


Our regression model is specified as follows:

```{r}
model.multivar <- '
  
  # regressions
  APst02 + Problems + Selfesteem ~ DS02 + LWst02 
  
  # residual variances and covariances
  APst02 + Problems + Selfesteem ~~ APst02 
  Problems + Selfesteem ~~ Problems
  Selfesteem ~~ Selfesteem
  
  # variances and covariances of predictors  # *
  DS02 + LWst02 ~~ DS02
  LWst02 ~~ LWst02 
  
  # intercepts / means
  APst02 + Problems + Selfesteem ~ 1  # intercept for each dependent
  DS02 + LWst02 ~ 1                   # means of predictors # *
'
```

*: This code is not needed to estimate the parameters, but it is needed such that lavaan will count these parameters correctly (see Exercise 3c).
 
Note: Instead of specifying intercepts and means in the model, one can include 'meanstructure=TRUE' in the lavaan function.

**Fit the regression model using the lavaan function:** 

```{r, message = FALSE}
if (!require("lavaan")) install.packages("lavaan", dependencies = TRUE)
library(lavaan)
#`fit_multivar <- lavaan(model = model.multivar, data = data_multivar)`
```

Because of the missing data, we will use FIML (as explained later in the course):

```{r, warning=FALSE}
fit_multivar <- lavaan(model = model.multivar, data = data_multivar,
                       missing='fiml', fixed.x=FALSE) # Specify FIML
```


**Technical output**

```{r}
lavInspect(fit_multivar)
```

In this output, we can see the numbers 1 to 20. This means that 20 parameters are estimated.

Bear in mind that the values are parameter labels, not parameter values.

Parameters 1 to 6 are the parameters in beta, which reflect regression coefficients. The rows denote the outcomes and the columns the predictors. Thus, in a way, each row denotes a regression equation. For instance, parameter 1 is the regression coefficient of DS02 predicting APst02 (conditional on the other predictor: LWst02).

Parameters 7 to 15 are the parameters in psi, where parameters 7 to 12 belong to the outcomes and thus reflect residual (co)variances and parameters 13 to 15 belong to the predictors and thus reflect the (co)variances of the predictors (which will equal the sample (co)variances here).

Parameters 16 to 20 are the parameters in alpha, where parameters 16 to 18 belong to the outcomes and thus reflect the intercepts in the regression equation and parameters 19 and 20 belong to the predictors and thus reflect the means of the predictors (which will equal the sample means here).

This is indeed the multivariate regression model we intended to fit.

Note that you could also inspect the path diagram of the fitted model for some insight. Such a path diagram (plot) is given at the end of this answer section. You can compare this path diagram to the one you created in Exercise 2a.


**Ouput**

```{r}
summary(fit_multivar, fit.measures = TRUE, ci = TRUE, rsquare = TRUE)
```
Note that 

- The residual variances are denoted by using a "." before the variable name. Here: .APst02, .Problems, and .Selfesteem 

  The variances are denoted without a ".". Here: DS02 and LWst02

- The intercepts are denoted by using a "." before the variable name. Here: .APst02, .Problems, and .Selfesteem

  The means are denoted without a ".". Here: DS02 and LWst02


**Parameter estimates**

To obtain the unstandardized regression parameters, use:
```{r}
coef(fit_multivar) 
```

To obtain all unstandardized parameters (with extra information), use:
```{r}
parameterEstimates(fit_multivar)
```

To obtain all **standardized parameters** (with extra information), use:           
```{r}
standardizedSolution(fit_multivar)
```


**Fit measures**

You can obatin many model fit measures, via:
```{r}
fitMeasures(fit_multivar)   
```

The R-square for all dependent variables can be asked via:
```{r}
inspect(fit_multivar, 'r2') 
```


**Possible conclusions:**

- The preidctors digit span (*DS02*) and letters words (*LWst02*) significantly predict all three outcomes.

- The two predictors positively relate to *APst02* and *Selfesteem*, and they relate negatively to *Problems*.

- The two predictors explain 36.5%, 5.2%, and 1.1% of the oucomes *APst02, Problems,* and *Selfesteem*.
So, quite a lot of the variance in APst02 is explained.
In all, we do miss relevant other predictors.

- When you are the expert (and look at the scale of the data), you can judge whether the effect are relevant or not.

- When looking at the absolute values of the standardized effects, *LWst02* is a more important predictor than *DS02* for *APst02* and *Problems*; while *DS02* is a more (or equally) important predictor than (as) *LWst02* for *Selfesteem*.

Note that the zero chi-square implies that the model is fully saturated: meaning that the theoretical covariance matrix is a function of as many parameters as there are distinct variances and covariances in the covariance matrix (i.e., the number of diagonal elements and the number of elements on one side of the diagonal). You have essentially re-interpreted the covariance matrix, and this may be quite meaningful. Then, what you have is essentially a regression model, and you must evaluate your model in those terms, that is: 

*Are the estimated relationships strong?* 

*Are the direction of the parameter estimates consistent with theory?* 

*Are any assumptions violated (note that this should be checked before doing the analysis)?*

Any basic regression text can guide you.


**Plot**

With lavaanPlot:
```{r, message = FALSE}
if (!require("lavaanPlot")) install.packages("lavaanPlot") 
library(lavaanPlot)
lavaanPlot(model = fit_multivar, 
           node_options = list(shape = "box", fontname = "Helvetica"),
           edge_options = list(color = "grey"), coefs = T, stand = T, covs = T)
```

Alternatively, with tidySEM:
```{r, message = FALSE, warning=FALSE}
if (!require("tidySEM")) install.packages("tidySEM")
library(tidySEM)
graph_sem(fit_multivar)
```

&nbsp;

&nbsp;

# Exercise 3: Path analysis

The data for this exercise is about corporal punishment[^3], which can be defined as the deliberate infliction of pain as retribution for an offence, or for the purpose of disciplining or reforming a wrongdoer or to change an undesirable attitude or behavior. Here, we are interested in how corporal punishment influences children's psychological maladjustment. The data come from 175 children between the ages of 8 and 18. The Physical Punishment Questionnaire (PPQ) was used to measure the level of physical punishment that was experienced.

In this exercise, we focus on predicting psychological *maladjustment* (higher score implies more problems) by perceived *rejection* (e.g., my mother does not really love me; my mother ignores me as long as I do nothing to bother her; my mother goes out of her way to hurt my feelings). Furthermore, *rejection* is predicted by perceived *harshness* (0 = never punished physically in any way; 16 = punished more than 12 times a week, very hard) and perceived *justness* (2 = very unfair and almost never deserved; 8 = very fair and almost always deserved). 

The data consists of a covariance matrix taken from a published paper and can be found in the file **CorPun.dat** or **CorPun.txt**. Bear in mind that, besides analyzing a data set you collected yourself, in lavaan (and Mplus) it is possible to base your analyses on a covariance or correlation table/matrix (and this is the reason why reviewers always ask you to include it). When using lavaan, you can use the following code

`lower <- scan("CorPun.txt")`

`CovMx <- getCov(lower, names = c("harsh", "just", "reject", "maladj"))`

[^3]: Rohner, R. P., Bourque, S. L., & Elordi, C. A. (1996). Children's perceptions of corporal punishment, caretaker acceptance, and psychological adjustment in a poor, biracial southern community. Journal of Marriage and the Family, 58, 842-852.

## Exercise 3a

Make a drawing of the statistical model about corporal punishment; write down which parameters (e.g., regression paths, covariances, residuals) you expect to be estimated; and number these parameters. 


<details><summary><b>Click to show answers</b></summary>

&nbsp;

In the (zipped) Solutions folder, you can find the file "Ex. 3 - Path diagram.png", which shows such a path diagram. 


**Number of parameters**

The parameters you expect to be estimated are: 

- 2 x variances (*harsh*, *just*)

-	1 x covariance (*harsh* with *just*)

-	3 x regression paths 

-	2 x residual variances (*reject* and *maladj*) 

-	2 x means (*just* and *harsh*, if means are available in the data)

-	2 x intercepts (*reject* and *maladj*, if means are available in the data)

Thus, 8 in total without means or 12 with means.

&nbsp;


## Exercise 3b

Write your lavaan code for this model based on the drawing you made in 3a and run it (after reading in your covariance matrix data). 

Note that since you are using a covariance matrix as the input file, you should indicate this - as well as the number of observations - in the lavaan function using:

`sample.cov = <name covariance matrix>`

`sample.nobs = 175`



<details><summary><b>Click to show answers</b></summary>

&nbsp;

For those interested, the corresponding Mplus in- and output files can be found in the subfolder 'Solutions\\Ex.3\\Mplus'. 

**Data**

Note: If you have no full dataset, but you do have a sample covariance matrix, you can still fit your model. If you wish to add a mean structure, you need to provide a mean vector too. Importantly, if only sample statistics are provided, you must specify the number of observations that were used to compute the sample moments (`sample.nobs = `). 
The following example illustrates the use of a sample covariance matrix as input (`sample.cov = `).

**Read in data: a covariance matrix**

```{r}
lower <- scan("CorPun.txt")
CovMx <- getCov(lower, names = c("harsh", "just", "reject", "maladj"))
```

**Specify the path model**

Now, let us run the model 

- predicting psychological maladjustment by perceived rejection. 

- predicting rejection by perceived harshness and perceived justness. 

Our path model is specified as follows:

```{r}
model.path <- '
  
  # regressions
  maladj ~ reject 
  reject ~ harsh + just
  
  # residual variances
  reject ~~ reject 
  maladj ~~ maladj
  #
  # variances and covariances of predictors # *
  harsh + just ~~ harsh
  just ~~ just
  
  ## intercepts - if we would have mean value as input too
  ## or use meanstructure=TRUE in the lavaan function
  #maladj ~ 1
  #reject ~ 1 
  #
  # means of predictors # *
  #harsh + just ~ 1
'
```

*: This code is not needed to estimate the parameters, but it is needed such that lavaan will count these parameters correctly (see Exercise 3c).

**Fit the regression model using the lavaan function:**

```{r, message = FALSE}
if (!require("lavaan")) install.packages("lavaan", dependencies = TRUE)
library(lavaan)
```

```{r}
fit_path <- lavaan(model = model.path, 
                       sample.cov = CovMx, # Now, using a covariance matrix as input
                       sample.nobs = 175)  # In that case, the sample size is needed as well
```

&nbsp;


## Exercise 3c

Ask for the technical output to inspect which parameters are estimated. For a lavaan fit object, use the function `lavInspect()`. For those familiar with Mplus, this is comparable to asking for the TECH1 output in Mplus. 

In this output, you can find six matrices (nu, lambda, theta, alpha, beta, and psi) with numbers summing up to the total number of parameters that are estimated. This way you can check whether you analyzed the model you wanted (and you will discover this is not always the case). 



*Which of the parameters you listed in Exercise 3a should be in which matrix?*  
For example: the regression coefficient between *rejection* and *harshness* should be in the beta matrix. Write this down for each parameter. 


*Did you estimate all the parameters that you expected? If not, what should be changed to the input?*  

<details><summary><b>Click to show a hint</b></summary>

&nbsp;

**Hint 1**

- A first check is to see whether 'Number of model parameters' equals the number of unique elements in your variance-covariance matrix and, if available, your mean vector. 

- You can count the number of elements in your variance-covariance matrix by: 

  number of variables * (number of variables + 1) / 2. 

- If you also have means, 'Number of model parameters' should equal the number of elements in your variance-covariance matrix plus the number of variables. 

Thus, for our four variables without means, the number of unique elements in our variance-covariance matrix is: 4*5/2=10.

If we would have means, the number of sample statistics would be 4*5/2 + 4 = 14.  


**Hint 2**

You can also answer this question by means of a plot: e.g., using `lavaanPlot()` from the lavaanPlot package or `graph_sem()` from the tidysem package. This will show you all (means, intercepts,) regression paths, (residual) variances, and covariances. Notably, `lavaanPlot()` will not provide you with information about the means and intercepts (i.e., the nu & alpha matrices) nor the residual (co)variances. 

</details>

&nbsp;


<details><summary><b>Click to show answers</b></summary>

&nbsp;

For those interested, the corresponding Mplus in- and output files can be found in the subfolder 'Solutions\\Ex.3\\Mplus'. 

**Technical output**

The function lavInspect() returns a list of the model matrices that are used internally to represent the model:

```{r}
lavInspect(fit_path)
```


Which parameter is listed in which matrix:
 
- The regression coefficients belong to the beta matrix, 

- The variances and residual variances belong to the psi matrix. If you specified them!


Note (related to the remark denoted by * in Exercises 2c and 3b): 

- When there is no specification of the variances of harsh and just and their covariance, the technical output (here, psi) indicates that these 3 parameters are not estimated. 

- This does not affect the degrees of freedom and thus the test result! 

- In case you want to check this, run:

```{r}
model.path_check <- '
  
  # regressions
  maladj ~ reject 
  reject ~ harsh + just
  
  # residual variances
  reject ~~ reject 
  maladj ~~ maladj
  #
  # Check: Not specifying these variances and covariances
  # variances and covariances of predictors # *
  #harsh + just ~~ harsh
  #just ~~ just
  
  ## intercepts - if we would have mean value as input too
  ## or use meanstructure=TRUE in the lavaan function
  #maladj ~ 1
  #reject ~ 1 
  #
  # means of predictors # *
  #harsh + just ~ 1
'
fit_path_check <- lavaan(model = model.path_check, 
                   sample.cov = CovMx, # Now, using a covariance matrix as input
                   sample.nobs = 175) # In that case, the sample size is needed as well
summary(fit_path_check, fit.measures = TRUE, ci = TRUE, rsquare = TRUE)
lavInspect(fit_path_check)
```

Hence, always cross check that lavaan has understood what you wanted it to do by inspecting the model degrees of freedom, the technical output (even the starting values), and model results. When these pieces of information do not contradict each other and lavaan gives you exactly the output you expect, then you can move forward with interpreting the model parameter estimates. 


**Plot**

With lavaanPlot:
```{r, message = FALSE}
if (!require("lavaanPlot")) install.packages("lavaanPlot") 
library(lavaanPlot)
lavaanPlot(model = fit_path, 
           node_options = list(shape = "box", fontname = "Helvetica"),
           edge_options = list(color = "grey"), coefs = T, stand = T, covs = T)
```

Alternatively, with tidySEM:
```{r, message = FALSE}
if (!require("tidySEM")) install.packages("tidySEM")
library(tidySEM)
graph_sem(fit_path)
```

&nbsp;



## Exercise 3d

Interpret the output.

*Write down the chi-square statistic and its degrees of freedom from the model fit information. Also inspect other model fit measures.*


<details><summary><b>Click to show answers</b></summary>

&nbsp;

**Output**

```{r}
summary(fit_path, fit.measures = TRUE, ci = TRUE, rsquare = TRUE)
```

Note that 

- the residual variances are denoted by using a "." before the variable name of the outcome / endogenous variable. Here: .reject and .maladj 

- the variances are denoted without a ".", so just the variable name of the predictor / exogenous variable. Here: harsh and just

You may notice that the variances of the predictors (hars and just) differ from the sample variances (i.e., the ones in the input file); e.g., 4.514 vs 4.54. This is because of rescaling. If you want them to be the same, then use 'sample.cov.rescale = FALSE'; as is done in the code below. Notably, this does not effect the other output, like the regression estimates.

```{r, message = FALSE}
fit_path_x <- lavaan(model = model.path, 
                         sample.cov = CovMx, 
                         sample.cov.rescale = FALSE, # this says do not rescale
                         sample.nobs = 175) 
summary(fit_path_x, fit.measures = TRUE, ci = TRUE, rsquare = TRUE)
```
As you can see, the (residual) variances changed and the variances of the predictors now equal the ones from the sample. Additionally, the other output, like the regression parameters, did not change.  
Next, I continue with the fitted model 'fit_path' (i.e., the model with rescaling).

  
*The chi-square statistic and its degrees of freedom:*

`Model Test User Model:`

`Test statistic                                 1.546`

`Degrees of freedom                                 2`

`P-value (Chi-square)                           0.462`


**Parameter estimates**

To obtain the unstandardized regression parameters, use:
```{r}
coef(fit_path) 
```

To obtain all unstandardized parameters (with extra information), use:
```{r}
parameterEstimates(fit_path)
```

To obtain all **standardized parameters** (with extra information), use:           
```{r}
standardizedSolution(fit_path)
```


**Fit measures**

You can obatin many model fit measures, via:
```{r}
fitMeasures(fit_path)   
```

You can ask for specific model fit measures by using:
```{r}
fitMeasures(fit_path, c("chisq", "cfi", "tli", "rmsea","srmr")) 
```

To obtain the R-square, that is, $R^2$, for all dependent variables, use:

```{r}
inspect(fit_path, 'r2')
```


Notes:

1. The 'chisq' or 'the Test Statistic for the Model Test User Model' is the Chi-square statistic obtained from the maximum likelihood statistic. 

2. CFI is the Comparative Fit Index.
    * Values can range between 0 and 1.
    * Values greater than 0.90, or conservatively 0.95, indicate good fit.

3. TLI is the Tucker Lewis Index
    * Values can range between 0 and 1. If it is greater than 1, it should be rounded to 1.
    * Values greater than 0.90 indicating good fit. 
    * If the CFI and TLI are less than one, the CFI is always greater than the TLI.

4. RMSEA is the root mean square error of approximation. 
    * You also obtain a p-value of close fit, that is, for the test whether RMSEA < 0.05.
    * If you reject the model, it means your model is not a close fitting model.


&nbsp;